training_config:
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 16
  num_train_epochs: 3
  dataloader_num_workers: 8
  # fp16: true
  bf16: true
  optim: "adamw_torch"
  learning_rate: 3.0e-4
  logging_steps: 10
  evaluation_strategy: "steps"
  save_strategy: "steps"
  save_steps: 250
  # do_train: False              # 学習を行わない
  # do_eval: True            # 評価のみを行う
  logging_dir: "./log"         # ログの保存場所
  eval_steps: 10000
  # save_steps: 1000
  save_total_limit: 5
  deepspeed: ./configs/deepspeed/ds_config_zero3.json
  output_dir: /raid/moriy/model/heron/
  report_to: "wandb"

model_config:
  fp16: true
  #pretrained_path:  # None or path to model weight
  model_type: video_blip
  language_model_name: daryl149/llama-2-7b-chat-hf
  num_image_with_embedding: 16 # 1 for image, otherwise for number of video sequences
  max_length: 128
  keys_to_finetune: []
  keys_to_freeze:
    - vision_model
    - language_model

  use_lora: false

dataset_config_path: 
  - ./configs/datasets/soccernet.yaml
